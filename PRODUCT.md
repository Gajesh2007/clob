### The Architecture: A Hybrid Streaming/Batching Model

The system is split into two distinct but connected planes: the **Real-Time Execution Plane** (what traders interact with) and the **Asynchronous Settlement Plane** (what provides verifiability).

### I. The Real-Time Execution Plane (The "Hot Path")

This is the system's nervous system, engineered for absolute minimum latency. It is a geographically distributed but logically centralized system operated by the Sequencer entity.

**A. User Interaction & Ingress:**

1.  **Co-location Facilities:** The Sequencer operates in major financial data centers (e.g., Equinix NY4 in New Jersey, LD4 in London). Trading firms rent rack space to place their servers feet away from the Sequencer's hardware.
2.  **Network Ingress:**
    *   **Hardware:** Enterprise-grade, low-latency network switches (e.g., Arista, Mellanox). Fiber optic cross-connects are established between trader racks and the Sequencer's ingress switches.
    *   **Protocols:**
        *   **Order Entry:** A simple, custom binary protocol (like NASDAQ's OUCH) over TCP for submitting signed orders, cancels, and modifications.
        *   **Market Data:** A real-time "firehose" feed of all public order book events (like NASDAQ's ITCH) is broadcast via UDP Multicast.
3.  **Load Balancers & Ingress Nodes:**
    *   A fleet of simple ingress nodes receives the raw TCP connections. Their only job is to terminate the connection and immediately forward the raw, signed message payload to the next stage.
    *   **Hardware:** Standard 1U servers with high-end Network Interface Cards (NICs) capable of kernel bypass for low latency.

**B. The Pre-Processing Pipeline:**

1.  **The Verification Farm:**
    *   A large, dedicated cluster of servers whose sole purpose is to validate signatures. This is a crucial offload from the main matching engine.
    *   Incoming messages from the ingress nodes are distributed across this farm.
    *   **Hardware:** CPU-dense servers (e.g., AMD EPYC with 64/128 cores per server) to maximize parallel signature verification.
2.  **Internal Message Bus:**
    *   Once a message is validated, it is placed onto an ultra-low-latency, in-memory message queue. This is not a generic system like Kafka, but likely a custom, proprietary implementation using RDMA (Remote Direct Memory Access) for nanosecond-level message passing between servers.

**C. The Matching Engine Core:**

This is the heart of the system. It is a single, stateful C++ application running on a beast of a machine.

1.  **The "Shared-Nothing" Application:**
    *   The application is multi-threaded, with each thread pinned to a specific CPU core (**CPU Affinity**).
    *   Each thread is responsible for a partition of the market (e.g., Thread #1 handles ETH/USD, Thread #2 handles BTC/USD, etc.).
    *   Each thread maintains its own order book in a memory region that no other thread touches, eliminating the need for slow cross-thread locks.
2.  **Execution Loop:**
    *   Each thread continuously pulls validated orders for its assigned markets from the internal message bus.
    *   It executes the order against its in-memory book (microseconds).
    *   It immediately writes the result (a trade, a new resting order) to two places simultaneously:
        *   An outgoing queue for market data and direct trade confirmations.
        *   A sequential, append-only **"Execution Log"** on a RAM disk or NVMe drive. This log is the raw material for the Settlement Plane.
3.  **Hardware:**
    *   A single, top-of-the-line server.
    *   **CPU:** A processor with the highest possible single-core clock speed and large L3 cache (e.g., an Intel Xeon "Max" Series or AMD EPYC with 3D V-Cache).
    *   **RAM:** Terabytes of the fastest available DRAM.
    *   **Storage:** The Execution Log is written to a RAM disk first, then flushed to high-end NVMe drives.
    *   **TEE:** The entire matching engine process runs inside a Trusted Execution Environment (Intel SGX or AMD SEV).

---

### II. The Asynchronous Settlement Plane (The "Warm Path")

This plane runs in the background. Its job is to take the `Execution Log` generated by the Hot Path and make it permanently available and verifiable without ever slowing down execution.

**A. Data Handling & Availability:**

1.  **The Data Pump:** A dedicated service continuously reads from the `Execution Log`.
2.  **Batching for DA:** It chunks the log data into batches (e.g., 10-50 MB). This is not a time-based batch, but a size-based one.
3.  **Dispersal to EigenDA:** Each chunk is streamed to the **EigenDA** network for storage and dispersal. This ensures data availability. The goal is to continuously feed EigenDA at a rate that keeps up with the Execution Log's growth, targeting a sustained throughput of up to 100 MB/s during peak times.

**B. State Commitment & Verification:**

1.  **Periodic Checkpointing:** Every 1-2 seconds, the main Matching Engine application (running in the TEE) performs a non-blocking checkpoint. It records:
    *   The current `state_root` (the Merkle root of all user accounts and order books).
    *   The `log_entry_ID` of the last transaction included in this checkpoint.
    *   A Merkle root of all the transactions processed since the last checkpoint.
2.  **TEE Attestation:** The TEE generates a cryptographic attestation for this checkpoint. This is a signed statement from the hardware chip itself, proving that the specific, audited code produced this state transition.
3.  **L1 Submission:** The Sequencer submits this TEE attestation, along with pointers to the relevant data blobs on EigenDA, to a **Settlement Contract on the L1 blockchain (Ethereum)**.

---

### III. How Independent Parties Verify the System

Verification is a multi-layered process, allowing for different levels of scrutiny.

**Level 1: Real-Time Soft Verification (For Traders)**
*   Traders verify the system's integrity in real-time by consuming the public ITCH market data feed. They can reconstruct the order book themselves and see that their own orders are processed correctly relative to the public state. This is a soft check against front-running within the stream.

**Level 2: Light Verification (Checking the Checkpoints)**
*   Anyone can run a light node. They query the L1 Settlement Contract to get the latest finalized `state_root` and TEE attestation.
*   They verify the signature on the TEE attestation against Intel/AMD's public keys.
*   They check that the `code_hash` in the attestation matches the known, open-source hash of the matching engine software.
*   This proves that a trusted piece of hardware ran the correct software to produce the latest state, but it doesn't re-execute the transactions.

**Level 3: Full Verification (The "Distrustful" Verifier)**
*   A dedicated verifier, or any sufficiently motivated user, can perform a full, independent audit.
*   **Step 1:** Fetch a finalized checkpoint (e.g., `state_root_N`) from the L1 contract.
*   **Step 2:** Fetch the *previous* finalized checkpoint (`state_root_N-1`).
*   **Step 3:** Go to **EigenDA** and download all the raw `Execution Log` data chunks between the two checkpoints.
*   **Step 4:** Re-run the open-source matching engine software locally, starting from `state_root_N-1` and feeding it all the downloaded transactions.
*   **Step 5:** Compare the resulting state root from their local execution with `state_root_N` from the L1 contract.
*   **Result:** If they match, the system's integrity is fully verified. If they don't, irrefutable proof of fraud exists, which could be used to trigger social slashing or legal action against the Sequencer operator.

This multi-level verification process allows for a spectrum of trust, from the real-time sanity checks performed by HFTs to the full, painstaking audits performed by watchdogs, all while maintaining the core promise of a verifiable, high-performance CLOB.
